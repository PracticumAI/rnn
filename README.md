Resources and presentations for the Recurrent Neural Network workshop.

***
#### Links

[RNN Article](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)

***
So for an RNN workshop, you need to stick to recurrent concepts completely, I think RNN-LSTM-GRU is what you touch on in an RNN workshop. 

Attention would be worth bringing up in the RNN workshop. Attention in an LSTM was one of the first big breakthroughs. And attention is what a transformer is so it’s an idea to a concept used for transformers which can be touched on more in a transformer class. 

1D conv net should not be brought up in an RNN class, rather it can be brought up in a CNN workshop. 1D works like 2D so it’s more intuitive than throwing it in to a recurrent structure which do not correlate between. 

So in summary:

RNN-LSTM-GRU-Attention for an RNN workshop. 

1D convnet introduced in a CNN workshop. 

Transformer not introduced, the entire network, until intermediate workshop. Talking about attention in the RNN will work. 

