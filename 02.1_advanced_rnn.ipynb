{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Practicum AI Logo image](https://github.com/PracticumAI/practicumai.github.io/blob/main/images/logo/PracticumAI_logo_250x50.png?raw=true)\n",
    "***\n",
    "# *Practicum AI:* RNN - Advanced RNN\n",
    "\n",
    "This exercise was adapted from Baig et al. (2020) <i>The Deep Learning Workshop</i> from <a href=\"https://www.packtpub.com/product/the-deep-learning-workshop/9781839219856\">Packt Publishers</a> (Exercises 6.01 - 6.05, page 269)\n",
    "\n",
    "In this exercise, we will explore several types of Recurrent Neural Network (RNN) models for sentiment classification. These models include a plain RNN, variations of the plain RNN, a Long Short-Term Memory (LSTM) model, a Gated Recurrent Unit (GRU) model, a bidirectional RNN, and a stacked RNN.\n",
    "\n",
    "We first define the architecture for each of these models and then evaluate their performance on the test data. This allows us to compare the performance of each model and determine which one is the most effective for this particular task.\n",
    "\n",
    "To finish this exercise, follow these steps:\n",
    "\n",
    "#### Data preparation\n",
    "##### Loading the Data - page 269\n",
    "\n",
    "To import the dataset, we will use the `imdb.load_data` function from the keras.datasets module.  A vocabulary size must be specified and passed to this function for it to work properly.  The dataset is automatically tokenized and split into training and test sets for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gkLq3SIsMGo",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zi848WzKsMGx"
   },
   "outputs": [],
   "source": [
    "vocab_size = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UiApCeTLsMG4"
   },
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a quick look at the *X_train* variable, to better understand the dataset and what needs to be done to prepare it for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 87
    },
    "colab_type": "code",
    "id": "TN7tBiySAZfh",
    "outputId": "d88f6934-929b-453e-89f3-10595ed93347"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'list'>\n",
      "[1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, 2, 32, 85, 156, 45, 40, 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 2, 16, 3804, 8, 4, 226, 65, 12, 43, 127, 24, 2, 10, 10]\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train))\n",
    "print(type(X_train[5]))\n",
    "print(X_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *X_train* variable is a numpy array, where each element of the array is a list representing the text for a single review. Instead of being in raw text form, the terms in the text are represented as numerical tokens.\n",
    "\n",
    "##### Staging and pre-processing our data - page 271\n",
    "Because sequence length varies significantly, we must ensure that they are all the same length before being fed to the model. To do this, we use the `pad_sequences` function from the `keras.preprocessing.sequence` module. This function allows us to specify a maximum sequence length, and it pads or truncates any sequences shorter or longer than that with zeros.  For this exercise, we set maximum sequence length to 200. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DppWi4RiAD6c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import preprocessing\n",
    "\n",
    "maxlen = 200\n",
    "\n",
    "X_train = preprocessing.sequence.pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "colab_type": "code",
    "id": "8OB8O_uxbjXX",
    "outputId": "8922b71b-de5d-47a5-8b6e-e6a04d234349"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    1  778  128   74   12  630  163   15    4 1766 7982\n",
      " 1051    2   32   85  156   45   40  148  139  121  664  665   10   10\n",
      " 1361  173    4  749    2   16 3804    8    4  226   65   12   43  127\n",
      "   24    2   10   10]\n"
     ]
    }
   ],
   "source": [
    "print(X_train[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZN2dIJSqtGq"
   },
   "source": [
    "***\n",
    "#### Exercise 6.01: (Student) - page 276\n",
    "\n",
    "#### Plain RNN Model for Sentiment Classification\n",
    "\n",
    "To classify the sentiment by a plain RNN model, this process will involve three steps:\n",
    "\n",
    "*  **First**: Define a sequential RNN model for sentiment classification.\n",
    "*  **Second**: Add embedding, RNN, dropout, and dense layers to the base model created in step 1. \n",
    "*  **Third**: Check the accuracy of the predictions on the test data to assess how well the model generalizes.\n",
    "\n",
    "[Basic structure of Recurrent Neural Network](https://commons.wikimedia.org/wiki/File:Recurrent_neural_network_unfold.svg)\n",
    "![image](images/Recurrent_neural_network_unfold.svg.png)\n",
    "\n",
    "#### 1. Import requisite libraries and set seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qGW0y8_DBog0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Import Keras libraries and initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CpNqHXVtqtGm"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Flatten, Dense, Embedding, SpatialDropout1D, Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Rh1AypLqtGo"
   },
   "outputs": [],
   "source": [
    "model_rnn = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Specify the embedding layer\n",
    "\n",
    "To define the input and output dimensions for our embedding layer, we need to set the input_dim parameter to the value of the *vocab_size* variable and the output_dim parameter to the desired number of dimensions.\n",
    "\n",
    "For example, if we set the input_dim to the value of vocab_size and the output_dim to 32, \n",
    "this will create an embedding layer with vocab_size input dimensions and 32 output dimensions. The input dimensions represent the size of the vocabulary, while the output dimensions represent the number of dimensions that the embedding layer will reduce the input to.\n",
    "\n",
    "```python\n",
    "model_rnn.add(Embedding(vocab_size, output_dim=32))\n",
    "model_rnn.add(SpatialDropout1D(0.4))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Add a simple RNN layer with 32 neurons\n",
    "\n",
    "```python\n",
    "model_rnn.add(SimpleRNN(32))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vBWXz8BNsMI-"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Add a dropout layer with 40% dropout\n",
    "\n",
    "```python\n",
    "model_rnn.add(Dropout(0.4))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xr8Hlx5LqtGu"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Add a dense layer\n",
    "\n",
    "```python\n",
    "model_rnn.add(Dense(1, activation = 'sigmoid'))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z-bdj24LqtGx"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Compile the model and view its summary\n",
    "\n",
    "```python\n",
    "model_rnn.compile(loss  = 'binary_crossentropy',\n",
    "              optimizer = 'rmsprop',\n",
    "              metrics   = ['accuracy'])\n",
    "\n",
    "model_rnn.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yPqnUhtaqtG0"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown here, we see that the majority of parameters are in the embedding layer, with 256,000 out of 278,241 total parameters. This is because we learn the embedding matrix during training, which has a dimensionality of `vocab_size(8000)*output_dim(32)`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Fit (train) the model\n",
    "\n",
    "To fit our model to the training data, we use the `fit` method and specify the following hyperparameters:\n",
    "\n",
    "* *batch_size*: The number of samples per gradient update.\n",
    "* *epochs*: The number of times the model will cycle through the entire dataset.\n",
    "\n",
    "We can also specify a validation split of 0.2 which reserves 20% of the training data for the validation step of the training process.  For example, to fit the model on the training data with a batch size of 128 for 10 epochs and a validation split of 0.2, we use the following code:\n",
    "\n",
    "```python\n",
    "history_rnn = model_rnn.fit(X_train, y_train, batch_size = 128, validation_split = 0.2, \n",
    "                            epochs = 10)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "_Yiecp9I9aEH",
    "outputId": "7b35dc9e-6195-47c8-e384-319e992d99a1"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the training output, we see that the validation accuracy reaches about 85.16%. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Make predictions on the test data using predict_classes()\n",
    "\n",
    "```python\n",
    "y_test_pred = model_rnn.predict_classes(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "id": "AZ3M8eCBsMJE",
    "outputId": "b25acea6-76ac-4576-c7ee-0a66ee700dda"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z7N3UHiJsMIQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_LPaIc2kB60f",
    "outputId": "5244afec-f790-4a6e-b32d-5469f6264795"
   },
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, the test accuracy is 84.15%. The model is performing fairly well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fw-43e0UqtHC"
   },
   "source": [
    "***\n",
    "##### Making Predictions on Unseen Data - page 280\n",
    "\n",
    "Now that you have trained the model on some test data and assessed its performance, the next step is to see how well the model performs with new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "75sXUR-oqtHC"
   },
   "outputs": [],
   "source": [
    "inp_review = \"An excellent movie!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sentiment in the text is positive. If the model is working well, it should predict the sentiment as positive.\n",
    "\n",
    "To test our trained model with new text data, we execute the following steps:\n",
    "\n",
    "* Tokenize the text into its individual terms, normalize the case, and remove any punctuation.\n",
    "* Use a defined vocabulary for the data. We can load the vocabulary and the term-to-index mapping using the `get_word_index` method from the `imdb` module.\n",
    "* Create a vocab map that converts the tokenized sentence into a sequence of term indices by performing a lookup for each term and returning the corresponding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QfOINT8UqtHF"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQG3TJQRqtHH"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['an', 'excellent', 'movie']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_to_word_sequence(inp_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ubW2Fps_qtHJ"
   },
   "outputs": [],
   "source": [
    "word_map = imdb.get_word_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "m2hIKRjGqtHL"
   },
   "outputs": [],
   "source": [
    "vocab_map = dict(sorted(word_map.items(), key = lambda x: x[1])[:vocab_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, let's define a function that processes raw text and returns the corresponding sequence of integers.  We do this with the following steps:\n",
    "\n",
    "1. Apply the `text_to_word_sequence` utility to the text to tokenize it and normalize the case.\n",
    "2. Perform a lookup in the *vocab_map* dictionary to convert the tokenized text into a sequence of term indices.\n",
    "3. Return the corresponding sequence of integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L2Ab9YCrqtHO"
   },
   "outputs": [],
   "source": [
    "def preprocess(review):\n",
    "    inp_tokens = text_to_word_sequence(review)\n",
    "    seq = []\n",
    "    for token in inp_tokens:\n",
    "        seq.append(vocab_map.get(token))\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eiFIegqXqtHQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[32, 318, 17]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(inp_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the `predict` method to classify the sentiment. This method takes in a batch of new data and returns a prediction for each sample in the batch. The prediction will be a single value between 0 and 1, with 0 representing a negative sentiment and 1 representing a positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_BobIPVQqtHS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.predict_classes([preprocess(inp_review)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The out prediction is 1 (positive). Let's apply the function to another raw text review and supply it to the model for prediction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7rj0p-c8qtHU"
   },
   "outputs": [],
   "source": [
    "inp_review = \"Don't watch this movie - poor acting, poor script, bad direction.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PngvvxZtqtHX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_rnn.predict_classes([preprocess(inp_review)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction is 0, and the sentiment in the review is negative as we expect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JEZZYqpEsMJl"
   },
   "source": [
    "***\n",
    "#### Exercise 6.02: (Student) - page 288 \n",
    "#### LSTM-Based Sentiment Classification Model\n",
    "\n",
    "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network that is capable of learning order dependence in sequence prediction problems. This means that they are able to remember and make use of information from previous time steps when making predictions at future time steps.\n",
    "\n",
    "LSTM networks achieve this by using a more complex recurrent unit called the LSTM cell. This type of cell is able to store and manipulate information over longer periods of time, allowing the network to do a better job of capturing patterns and dependencies in the data.\n",
    "\n",
    "However, the increased complexity of the LSTM cell comes at a cost, as it requires more resources to train and run compared to simple recurrent units like the RNN cell. That is, LSTM networks are can be computationally intensive, but they are frequently more effective at learning complex data patterns.\n",
    "\n",
    "[Basic structure of Long Short-Term Memory Network](https://commons.wikimedia.org/wiki/File:Long_Short-Term_Memory.svg)\n",
    "![image](images/1600px-Long_Short-Term_Memory.svg.png)<br>\n",
    "\n",
    "Let's build a simple LSTM-based model to predict sentiment in our data.\n",
    "\n",
    "#### 1. Import the LSTM layer from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MKU9k7L8sMJo"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Instantiate a sequential model & add embedding/dropout layers\n",
    "\n",
    "```python\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(Embedding(vocab_size, output_dim=32))\n",
    "model_lstm.add(SpatialDropout1D(0.4))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v_zgvlz_qtHf"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Add an LSTM layer with 32 nodes\n",
    "\n",
    "```python\n",
    "model_lstm.add(LSTM(32))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wAMiP0Q4qtHi"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Add dropout and dense layers and then compile and summarize the model\n",
    "\n",
    "```python\n",
    "model_lstm.add(Dropout(0.4))\n",
    "model_lstm.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model_lstm.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = 'rmsprop',\n",
    "              metrics   = ['accuracy'])\n",
    "\n",
    "model_lstm.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fLaixjiaqtHl"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By examining the model summary, we see that the number of parameters in the LSTM layer is 8320. This is exactly four times the number of parameters in the plain RNN layer. LSTM models, as noted earlier, are more complex and that is reflected in the number of parameters we see here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Fit (train) the model\n",
    "\n",
    "Now, let's fit the model on the training data for 5 epochs with a batch size of 128. \n",
    "\n",
    "```python\n",
    "history_lstm = model_lstm.fit(X_train, y_train, batch_size = 128, validation_split = 0.2, \n",
    "                              epochs = 5)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "jXSS5OyOd03R",
    "outputId": "fccd2a09-7a8a-4b10-afd5-3dbda5a0afdc"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the increased complexity of the LSTM cell has improved performance. Here we see that the validation accuracy of the LSTM model is higher than that of the plain RNN model, indicating that the model is better able to generalize when faced with new data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Make predictions on the test data and print accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "do1S7DgwsMJ6"
   },
   "outputs": [],
   "source": [
    "y_test_pred = model_lstm.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "3uhbw_LUk1t-",
    "outputId": "37ab7340-32ce-450f-baae-9534a80e3550"
   },
   "outputs": [],
   "source": [
    "print(accuracy_score(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy we got for the test data is 87.05%, an improvement from the accuracy we got using plain RNNs at 84.15%. It looks like the extra parameters and the extra predictive power came in handy for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yyzsib0i0Vmk"
   },
   "source": [
    "***\n",
    "#### Exercise 6.03: (Student) - page 294\n",
    "\n",
    "#### GRU-Based Sentiment Classification Model\n",
    "\n",
    "Gated Recurrent Unit (GRU) is a type of recurrent neural network that can be used in place of Long Short-Term Memory (LSTM) networks in specific cases. GRUs are similar to LSTMs in that they are able to capture long-term dependencies in sequential data, but they use a simpler and more efficient type of recurrent unit.\n",
    "\n",
    "One advantage GRUs have over LSTMs is that they require less memory and are faster to train and run. This is because the GRU cell has fewer parameters than the LSTM cell.  And as a result, it requires fewer resources.\n",
    "\n",
    "LSTMs and GRUs do equally well on most tasks.  In some cases, though, LSTMs may still be the better choice due to their ability to capture more complex patterns in the data.  So, you may want to experiment with both types of recurrent network to determine which one performs better on a particular task.\n",
    "\n",
    "[Basic structure of Gated Recurrent Unit](https://commons.wikimedia.org/wiki/File:Gated_Recurrent_Unit.svg)\n",
    "![image](images/1600px-Gated_Recurrent_Unit.svg.png)<br>\n",
    "\n",
    "Let's build a simple GRU-based model to predict the sentiment of a review.\n",
    "\n",
    "#### 1. Import the GRU layer from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Goen0NR3uueL"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Instantiate the model and add embedding and dropout layers\n",
    "\n",
    "```python\n",
    "model_gru = Sequential()\n",
    "model_gru.add(Embedding(vocab_size, output_dim = 32))\n",
    "model_gru.add(SpatialDropout1D(0.4))\n",
    "```   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2A1Y2vOvuueQ"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Add a GRU layer with 32 nodes\n",
    "\n",
    "```python\n",
    "model_gru.add(GRU(32, reset_after = False))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mo5Uq26SqtH6"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Add dropout and dense layers - compile and summarize the model\n",
    "\n",
    "```python\n",
    "model_gru.add(Dropout(0.4))\n",
    "model_gru.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_gru.compile(loss  = 'binary_crossentropy',\n",
    "              optimizer = 'rmsprop',\n",
    "              metrics   = ['accuracy'])\n",
    "\n",
    "model_gru.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nZBqzjKjqtH8"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By examining the model summary, we see that the number of parameters in the GRU layer is 6240. This is approximately three times the number of parameters in the plain RNN layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Fit (train) the model\n",
    "\n",
    "Now, let's fit the model on the training data for 5 epochs with a batch size of 128.#### 5. Fit (train) the model\n",
    "\n",
    "```python\n",
    "history_gru = model_gru.fit(X_train, y_train, batch_size = 128, validation_split = 0.2, \n",
    "                            epochs = 4)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "RoyCrxH1qArB",
    "outputId": "6027bfd6-4fb3-4d6e-e898-805e04617abd"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training time for our GRU model took much longer than the plain RNN, but it was faster than the LSTM model. The validation accuracy is also better than the plain RNN and close to that of the LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Make predictions on the test data and print accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJVjdE3-uueU"
   },
   "outputs": [],
   "source": [
    "y_test_pred = model_gru.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1XhmgQdVsMKs",
    "outputId": "08612983-40b3-450b-b6da-6f61b0598992"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87156"
      ]
     },
     "execution_count": 74,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training and evaluating the GRU model on the test data, we see that its accuracy is similar to that of the LSTM model (87.06% vs 87.05%). This suggests that the GRU model is able to capture the important patterns and dependencies in the data and make accurate predictions, despite having fewer parameters than the LSTM model.\n",
    "\n",
    "This is an important point.  GRUs are often used as a simplified alternative to LSTMs, due to their ability to provide similar accuracy with fewer parameters. This makes them more efficient to train and run.  This is a useful property, especially when computational resources are limited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V4LEp8vE9vlJ"
   },
   "source": [
    "***\n",
    "#### Exercise 6.04: (Teacher) - page 299\n",
    "\n",
    "#### Bi-directional LSTM-Based Sentiment Classification Model\n",
    "\n",
    "Recurrent neural networks (RNNs), such as Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRUs), are powerful tools for processing sequential data and can achieve excellent results on a wide range of tasks. However, there are ways to make these models even more powerful by modifying their architecture.\n",
    "\n",
    "One such modification is the use of bidirectional RNNs. A bidirectional RNN is a type of neural network that processes the sequence information in both directions, either backward (from future to past) or forward (from past to future). This can be particularly useful for tasks such as machine translation, parts-of-speech tagging, name entity recognition, and word prediction, where understanding the context of a word or phrase is important.\n",
    "\n",
    "[Basic structure of bidirectional LSTM.](https://www.mdpi.com/2076-3417/11/17/8129/htm)\n",
    "<div>\n",
    "<img src=\"images/bidirectional_LSTM.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Now, let's apply a bidirectional LSTM-based model to our sentiment classification task.\n",
    "\n",
    "#### 1. Import the Bidirectional layer from Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJWbU-VF9vlL"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Bidirectional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Instantiate the model and add embedding and dropout layers\n",
    "\n",
    "```python\n",
    "model_bilstm = Sequential()\n",
    "model_bilstm.add(Embedding(vocab_size, output_dim = 32))\n",
    "model_bilstm.add(SpatialDropout1D(0.4))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VyvzqDuP9vlP"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Add a Bidirectional wrapper to an LSTM layer\n",
    "\n",
    "```python\n",
    "model_bilstm.add(Bidirectional(LSTM(32)))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mBffFS21qtIL"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Add dropout and dense layers - compile and summarize the model\n",
    "\n",
    "```python\n",
    "model_bilstm.add(Dropout(0.4))\n",
    "model_bilstm.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_bilstm.compile(loss = 'binary_crossentropy',\n",
    "              optimizer   = 'rmsprop',\n",
    "              metrics     = ['accuracy'])\n",
    "\n",
    "model_bilstm.summary()\n",
    "```           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dr0tJanmqtIN"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bidirectional LSTM layer has twice the number of parameters as the LSTM layer, with a total of 16,640 parameters. This is eight times the number of parameters in a plain RNN, which has a total of 8,320 parameters. It is not surprising that the bidirectional LSTM has more parameters given its increased complexity compared to the LSTM and plain RNN models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Fit (train) the model\n",
    "\n",
    "Now, let's fit the model on the training data for 5 epochs with a batch size of 128\n",
    "\n",
    "```python\n",
    "history_bilstm = model_bilstm.fit(X_train, y_train, batch_size = 128, validation_split = 0.2, \n",
    "                                  epochs = 4)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 319
    },
    "colab_type": "code",
    "id": "imWOyTg2q8gV",
    "outputId": "d1d2685d-73b5-4376-f5c2-4b4599bd246d"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that training a bidirectional LSTM model takes significantly longer than a regular LSTM model. Despite this, the validation accuracy of the bidirectional LSTM model appears to be similar to that of the LSTM "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Make predictions on the test data and print accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zv0Wz9X3rsM0"
   },
   "outputs": [],
   "source": [
    "y_test_pred = model_bilstm.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "f5W6wArirr_n",
    "outputId": "59bf7b54-56d9-4080-d5bd-66dd20bb6510"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.877"
      ]
     },
     "execution_count": 88,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the test data for the bidirectional LSTM model is 87.60%, which is a slight improvement over the accuracy of the LSTM model at 87.05%. It is possible to further tune the hyperparameters of the bidirectional LSTM model to potentially improve its performance even more. This demonstrates the potential of this powerful architecture to achieve high levels of accuracy on a variety of tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t8KSaCNgsMKw"
   },
   "source": [
    "***\n",
    "#### Exercise 6.05: (Student) - page 302\n",
    "\n",
    "#### Stacked LSTM-based Sentiment Classification Model\n",
    "\n",
    "An alternative approach to increasing the performance of RNNs is to use stacked RNNs. Stacking RNNs involves feeding the output of one RNN layer into another RNN layer, effectively creating a multi-layer RNN model. This can potentially improve the model's ability to learn and make more accurate predictions.\n",
    "\n",
    "[Basic structure of Stacked LSTM](https://medium.com/@amardeepchauhan/paradigms-of-various-lstm-networks-e95ef1d6caaa)\n",
    "\n",
    "<div>\n",
    "<img src=\"images/stacked_LSTM.png\" width=\"450\"/>\n",
    "</div>\n",
    "\n",
    "Let's build a stacked LSTM-based model by stacking two LSTM layers to predict sentiment in our data.\n",
    "\n",
    "#### 1. Instantiate the model\n",
    "\n",
    "```python\n",
    "model_stack = Sequential()\n",
    "model_stack.add(Embedding(vocab_size, output_dim = 32))\n",
    "model_stack.add(SpatialDropout1D(0.4))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkEljoMtw8VH"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Add an LSTM layer with 32 nodes\n",
    "\n",
    "Specify *return_sequences* as *True* in the LSTM layer. This will return the output of the LSTM at each time step, which can then be passed to the next LSTM layer.\n",
    "\n",
    "```python\n",
    "model_stack.add(LSTM(32, return_sequences = True))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6xiwd5zjqtIb"
   },
   "outputs": [],
   "source": [
    "# LSTM Layer 1 - return_sequences is True\n",
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Add a second LSTM layer with 32 nodes\n",
    "\n",
    "This time we don't need to return the sequence. You can either specify the *return_sequences* option as *False* or skip it altogether. des\n",
    "\n",
    "```python\n",
    "model_stack.add(LSTM(32, return_sequences = False))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NdrW_C-IqtId"
   },
   "outputs": [],
   "source": [
    "# LSTM Layer 2 - return_sequences is False\n",
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Add dropout and dense layers - compile and summarize the model\n",
    "\n",
    "```python\n",
    "model_stack.add(Dropout(0.5))\n",
    "model_stack.add(Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model_stack.compile(loss = 'binary_crossentropy',\n",
    "              optimizer  = 'rmsprop',\n",
    "              metrics    = ['accuracy'])\n",
    "\n",
    "model_stack.summary()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 353
    },
    "colab_type": "code",
    "id": "ZpQjiXUZqtIf",
    "outputId": "b522a857-e37b-4cbd-cfef-6034508d72b1"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the stacked LSTM model has the same number of parameters as the bidirectional model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Fit (train) the model\n",
    "\n",
    "Now, let's fit the model on the training data for 5 epochs with a batch size of 128.\n",
    "\n",
    "```python\n",
    "history_stack = model_stack.fit(X_train, y_train, batch_size=128, validation_split=0.2, \n",
    "                                epochs = 4)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 188
    },
    "colab_type": "code",
    "id": "6yFRu6wIsMK9",
    "outputId": "fd610b29-b559-499c-ff11-e453f426f0be"
   },
   "outputs": [],
   "source": [
    "# Code it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training stacked LSTM models takes less time than training bidirectional LSTM models. Despite this, the validation accuracy of the stacked LSTM model appears to be similar to that of the bidirectional LSTM model. This suggests that while stacked LSTM models may be more efficient to train, they do not necessarily sacrifice accuracy or performance compared to more complex models such as bidirectional LSTMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Make predictions on the test data and print accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bLBHL3QmsMLD"
   },
   "outputs": [],
   "source": [
    "y_test_pred = model_stack.predict_classes(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "6pXgTY5GsMLO",
    "outputId": "cb1cad43-90e3-4449-cda8-d1233ff160c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87572"
      ]
     },
     "execution_count": 108,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of 87.53% is a slight improvement over the LSTM model (87.05%) and is practically the same as that of the bidirectional model (87.60%).\n",
    "\n",
    "Now let's take a broader look at the situation and compare the models. Consider the table below, which compares five models in terms of parameters, training time, and test accuracy on our dataset.\n",
    "\n",
    "| Model | RNN layer parameters | Training time | Test accuracy |\n",
    "| --- | --- | --- | --- |\n",
    "| Plain RNN | 2,080 | Low | 84.15% |\n",
    "| LSTM | 8,320 | High | 87.05% |\n",
    "| GRU | 6,240 | Medium-High | 87.06% |\n",
    "| Bi-directional LSTM | 16,640 | Very High | 87.60% |\n",
    "| Stacked LSTM | 16,640 | Very High | 87.53% |\n",
    "\n",
    "According to the table, plain RNNs have the lowest number of parameters and shortest training times, but also have the lowest accuracy of all the models. LSTMs and GRUs perform better than plain RNNs, but their increased accuracy comes at the cost of longer training times and a larger number of parameters, increasing the risk of overfitting.\n",
    "\n",
    "The stacked and bidirectional approaches seem to offer incremental improvements in terms of predictive power, but this comes at the price of significantly longer training times and a larger number of parameters. Despite this, the stacked and bidirectional approaches yielded the highest accuracy, even on a small dataset.\n",
    "\n",
    "#### Summary \n",
    "\n",
    "In this exercise, we explored various versions of RNNs, including plain RNNs, LSTMs, and GRUs. We saw that plain RNNs are not practical for modeling long-range dependencies due to the vanishing gradient problem.  LSTMs, on the other hand, handle long sequences better but have many more parameters.  GRUs are a simpler alternative that work well on small datasets.  We then considered bidirectional and stacked RNNs.  These approaches have greatly improved the ability of RNNs to achieve state-of-the-art results on various tasks."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "06 RNNs on steroids v0.5.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Tensorflow-2.4.1",
   "language": "python",
   "name": "tensorflow-2.4.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
